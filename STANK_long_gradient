from pypylon import pylon
import cv2
import time
import pygame
import csv
import matplotlib.pyplot as plt

fly_name = "sleeping_male_GABOXADOL24_many_flies_gradient_mating3"

rawname = fly_name + "_raw.mp4"
moviename = fly_name + ".mp4"
csv_name = fly_name + ".csv"
plotname = fly_name + ".pdf"

# connecting to the first available camera
camera = pylon.InstantCamera(pylon.TlFactory.GetInstance().CreateFirstDevice())
camera.Open()

camera.Width = 600
camera.OffsetX.SetValue(228)

# binning_factor = 2  # Adjust this value as needed

# # Enable and set binning
# camera.BinningHorizontal.SetValue(binning_factor)
# camera.BinningVertical.SetValue(binning_factor)

camera.ExposureTime.SetValue(9000)
# Set the desired frame rate
frame_rate = 20

time_list = []
velocity_list = []

# Set the duration of the video directly IN SECONDS
video_duration = 1800

camera.AcquisitionFrameRateEnable.SetValue(True)
camera.AcquisitionFrameRate.SetValue(frame_rate)

# initialize background subtractor and bounding box variables
fgbg = cv2.createBackgroundSubtractorMOG2(history=1000, varThreshold=800)

bbox = None

# initialize variables for calculating velocity
prev_center = None
prev_time = time.time()
velocity = None

writer = cv2.VideoWriter(moviename, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (600, 1080))

raw_writer = cv2.VideoWriter(rawname, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (600, 1080))

pygame.init()


#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/10_seconds_mating_call.wav")
#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/10_seconds_300hz.wav")
#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/pink_noise_70_85_100.wav") #NOISE
pygame.mixer.music.load ("/home/joeh/Auditory_filtering_experiment/long/mating_call_70_85_100.wav") #MATING
#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/300hz_normal.wav")
#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/pink_noise.wav")
#pygame.mixer.music.load("/home/joeh/Auditory_filtering_experiment/long/650hz.wav")

converter = pylon.ImageFormatConverter()
converter.OutputPixelFormat = pylon.PixelType_BGR8packed
converter.OutputBitAlignment = pylon.OutputBitAlignment_MsbAligned

start_time = time.time()
elapsed_time = 0

# get some variables going
frame_count = 0

camera.StartGrabbing(pylon.GrabStrategy_LatestImageOnly)

counter = 0
play_count = 0
sound_time = []
audio_played_time = None

last_motion = 0
#pygame.mixer.music.play()

sound_played = False

while camera.IsGrabbing() and elapsed_time < video_duration:
    grabResult = camera.RetrieveResult(5000, pylon.TimeoutHandling_ThrowException)

    elapsed_time = time.time() - start_time

    if grabResult.GrabSucceeded():
        # access the image data
        image = converter.Convert(grabResult)
        img = image.GetArray()

        frame = img

    # apply background subtraction
    fgmask = fgbg.apply(img)

    kernel_size = (5, 5)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)
    kernel = cv2.GaussianBlur(kernel, kernel_size, 0)
    fgmask = cv2.filter2D(fgmask, -1, kernel)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50, 50))
    fgmask = cv2.dilate(fgmask, kernel, iterations=1)

    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    max_area = 100
    max_contour = None
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > max_area:
            max_area = area
            max_contour = contour

    min_width = 30
    min_height = 30
    max_width = 90
    max_height = 90

    if max_contour is not None:
        x, y, w, h = cv2.boundingRect(max_contour)
        if w >= min_width and h >= min_height:
            if w > max_width:
                w = max_width
            if h > max_height:
                h = max_height
            bbox = (x, y, w, h)

    if bbox is not None:
        center = (x + w // 2, y + h // 2)
        if prev_center is not None:
            current_time = time.time()
            dt = current_time - prev_time
            dx = center[0] - prev_center[0]
            dy = center[1] - prev_center[1]
            velocity = (dx / dt, dy / dt)
            prev_time = current_time
        prev_center = center

        if cv2.norm(velocity) > 80:
            time_since_motion = 0
            last_motion = time.time()

    else:
        time_since_motion += elapsed_time

    time_without_motion = time.time() - last_motion

    if cv2.norm(velocity) > 5:
        time_without_motion = 0

    # if elapsed_time >= 10 and not sound_played:
    #     if cv2.norm(velocity) < 500 and cv2.norm(velocity) > 100:
    #         sound_time.append(elapsed_time)
    #         pygame.mixer.music.play()
    #         print([sound_time[0]])
    #         sound_played = True
    #         cv2.putText(frame, "sound ON ", (50, 70),
    #                     cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            

    if time_without_motion >= 300 and time_without_motion < 300.2 and not sound_played:
        #if cv2.norm(velocity) < 500 and cv2.norm(velocity) > 100:
            sound_time.append(elapsed_time)
            pygame.mixer.music.play()
            print([sound_time[0]])
            sound_played = True
            cv2.putText(frame, "sound ON ", (50, 70),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            

    cv2.putText(frame, "Time: " + str(round(elapsed_time, 1)), (10, 70),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    raw_writer.write(img)

    cv2.putText(img, f"Time without motion: {(time_without_motion)}", (20, 500), cv2.FONT_HERSHEY_SIMPLEX, 1,
                (0, 0, 500), 2)

    if time_without_motion >= 300:
        cv2.putText(img, f"SLEEP DETECTED", (20, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 500), 2)

    if bbox is not None:
        cv2.rectangle(img, bbox, (0, 255, 0), 2)

        cv2.putText(img, "Velocity: {:.2f} pixels/sec".format(cv2.norm(velocity)), (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        writer.write(img)

    resized_img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5)
    cv2.imshow("Motion Detection", resized_img)
    cv2.resizeWindow("Motion Detection", 40, 480)

    if cv2.waitKey(1) == ord('q'):
        break

    if elapsed_time > 5:  # Filter out data points where time is below 5000 milliseconds
        time_list.append(elapsed_time)
        velocity_list.append(cv2.norm(velocity))

print(elapsed_time)

grabResult.Release()

camera.Close()
cv2.destroyAllWindows()
writer.release()
raw_writer.release()

with open(csv_name, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Time (milliseconds)', 'Locomotion (pixels/sec)', 'First Sound Time'])
    for i in range(len(time_list)):
        writer.writerow([round(time_list[i] * 1000, 1), round(velocity_list[i], 2), (sound_time[0]*1000)   ])

# Plot the data for Time above 5000 milliseconds
time_above_5000 = [t for t in time_list if t > 5]
velocity_above_5000 = [v for t, v in zip(time_list, velocity_list) if t > 5]

plt.plot(time_above_5000, velocity_above_5000)
#plt.xlim([5, 500])
plt.ylim([0, 800])
plt.xlabel("Time (s)")
plt.ylabel("Movement (p/s)")
plt.axvline(x=[sound_time[0]], color='green', label='axvline - full height')

plt.axvline(x=[sound_time[0] + 12.8], color='blue', linestyle='dashed', label='second sound time')

plt.axvline(x=[sound_time[0] + 25.6], color='blue', linestyle='dashed', label='second sound time')

plt.title("Locomotion / Time (Above 5000 ms)")
plt.savefig(plotname)
plt.show()
